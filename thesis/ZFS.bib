@online{mckusick_zfs_2015_presentation,
	title = {{ZFS} Internals Overview},
	url = {https://www.youtube.com/watch?v=IQp_FglfzUQ},
	author = {{McKusick}, Kirk},
	urldate = {2020-09-01},
	date = {2015-10-21},
}

@online{ahrens_openzfs_basics,
	title = {{OpenZFS} Basics},
	url = {https://www.youtube.com/watch?v=MsY-BafQgj4},
	author = {Ahrens, Matt and Wilson, George},
	urldate = {2020-08-29},
	date = {2018-05-14},
}

@online{ahrens_read_write,
	title = {Lecture on {OpenZFS} read and write code paths},
	url = {https://www.youtube.com/watch?v=ptY6-K78McY},
	author = {Ahrens, Matt},
	urldate = {2020-08-29},
	date = {2016-03-03},
}

@online{dawidek_closer_2008,
	title = {A closer look at the {ZFS} file system},
	url = {https://www.bsdcan.org/2008/schedule/attachments/58_BSDCan2008-ZFSInternals.pdf},
	shorttitle = {{ZFS} the Internals},
	note = {{BSDCan}},
	author = {Dawidek, Pawe≈Ç},
	urldate = {2020-12-20},
	date = {2008-05-16},
}

@incollection{zfs_terminology_fbsd,
	title = {19.8. {ZFS} Features and Terminology},
	booktitle = {FreeBSD Handbook},
	url = {https://www.freebsd.org/doc/handbook/zfs-term.html},
	author = {Rhodes, Tom and Jude, Allan and Reuschling, Benedict and Block, Warren},
	urldate = {2020-12-21},
}

@software{zfs,
    author = {Matt Ahrens et al.},
    title = {OpenZFS},
    url = {https://github.com/openzfs/zfs/tree/zfs-0.8.5},
    version = {0.8.5},
    date = {2020-10-06}
}

@software{zfs2,
    author = {Matt Ahrens et al.},
    title = {OpenZFS},
    url = {https://github.com/openzfs/zfs/tree/zfs-2.0.1},
    version = {2.0.1},
    date = {2021-01-06}
}

@software{linux,
    author = {Linus Torvalds et al.},
    title = {Linux},
    url = {https://www.kernel.org/},
    version = {5.4.65},
    date = {2020-09-12}
}

@software{newlinux,
    author = {Linus Torvalds et al.},
    title = {Linux},
    url = {https://www.kernel.org/},
    version = {5.10.4},
    date = {2020-12-30}
}
@software{fio,
    author = {Jens Axboe et al.},
    title = {Flexible I/O Tester},
    url = {https://github.com/axboe/fio},
    version = {3.25},
    date = {2020-12-04}
}

@article{lameter_numa_2013,
	title = {{NUMA} (Non-Uniform Memory Access): An Overview},
	volume = {11},
	issn = {15427730},
	url = {http://dl.acm.org/citation.cfm?doid=2508834.2513149},
	doi = {10.1145/2508834.2513149},
	shorttitle = {{NUMA} (Non-Uniform Memory Access)},
	pages = {40},
	number = {7},
	journaltitle = {Queue},
	shortjournal = {Queue},
	author = {Lameter, Christoph},
	urldate = {2020-06-10},
	date = {2013-07-01},
	langid = {english},
}

@article{dobson_linux_numa,
	title = {Linux Support for {NUMA} Hardware},
	journaltitle = {Proceedings of the Ottawa Linux Symposium},
	author = {Dobson, Matthew and Gaughen, Patricia and Hohnbaum, Michael and Focht, Erich},
	date = {2003-07-26},
	langid = {english},
	url = {https://www.kernel.org/doc/ols/2003/ols2003-pages-169-184.pdf},
}

@unpublished{lameter_slab_2014,
	title = {Slab allocators in the Linux Kernel},
	url = {https://events.static.linuxfound.org/sites/events/files/slides/slaballocators.pdf},
	author = {Lameter, Christoph},
	date = {2014-10-03},
	langid = {english},
}

@report{kochhar_optimal_2009,
	title = {Optimal {BIOS} Settings for High Performance Computing with {PowerEdge} 11G Servers},
	url = {https://i.dell.com/sites/content/business/solutions/whitepapers/en/Documents/11g-optimal-bios-settings-poweredge.pdf},
	pages = {28},
	institution = {Dell Product Group},
	author = {Kochhar, Garima and Liberman, Jacob},
	date = {2009-07-13},
	langid = {english}
}

@inproceedings {megiddo_dharmendra_ARC,
author = {Nimrod Megiddo and Dharmendra S. Modha},
title = {{ARC}: A Self-Tuning, Low Overhead Replacement Cache},
booktitle = {2nd {USENIX} Conference on File and Storage Technologies ({FAST} 03)},
year = {2003},
address = {San Francisco, CA},
url = {https://www.usenix.org/conference/fast-03/arc-self-tuning-low-overhead-replacement-cache},
publisher = {{USENIX} Association},
month = mar,
}


@article{bergstrom_stream,
  author    = {Lars Bergstrom},
  title     = {Measuring {NUMA} effects with the {STREAM} benchmark},
  journal   = {CoRR},
  volume    = {abs/1103.3225},
  year      = {2011},
  url       = {http://arxiv.org/abs/1103.3225},
  archivePrefix = {arXiv},
  eprint    = {1103.3225},
  timestamp = {Mon, 13 Aug 2018 16:47:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1103-3225.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{li_characterization_2013,
	title = {Characterization of Input/Output Bandwidth Performance Models in {NUMA} Architecture for Data Intensive Applications},
	doi = {10.1109/ICPP.2013.46},
	abstract = {Data-intensive applications frequently rely on multicore computer systems, in which Non-Uniform Memory Access ({NUMA}) is a dominant architecture. To transfer data into and out from these high-performance computers becomes a bottleneck, and thus it is crucial to understand their I/O performance characteristics. However, the complexity in {NUMA} architecture presents a new challenge in modeling its I/O access cost, and thus lead to difficulties in configuring proper processor and memory affinity. In this paper, we show that existing {NUMA} experimental methods and metrics are inappropriate on contemporary high-end systems. We characterize a state-of-the-art {NUMA} host, and propose, to the best of our knowledge, the first methodology to simulate I/O operations using memory semantics, and model the I/O bandwidth performance. Our methodology is thoroughly tested and validated by mapping multiple parallel I/O streams to different sets of hardware components ({CPU}, memory, network cards, and {SSDs}) and by measuring the performance of each mapping. The experimental results and analysis reveal that our methodology can dramatically reduce characterization workload, accurately estimate the overall I/O performance, and effectively mitigate resource contention among I/O tasks.},
	eventtitle = {2013 42nd International Conference on Parallel Processing},
	pages = {369--378},
	booktitle = {2013 42nd International Conference on Parallel Processing},
	author = {Li, T. and Ren, Y. and Yu, D. and Jin, S. and Robertazzi, T.},
	date = {2013-10},
	note = {{ISSN}: 2332-5690},
	keywords = {Bandwidth, Benchmark testing, Data transfer, Data Transfer, Hardware, Input/Output(I/O), {NUMA} effects, Performance evaluation, Performance model, Servers, Topology},
}

@article{song_evaluation_2017,
	title = {Evaluation of Performance Unfairness in {NUMA} System Architecture},
	volume = {16},
	issn = {1556-6064},
	doi = {10.1109/LCA.2016.2602876},
	abstract = {{NUMA} (Non-uniform memory access) system architectures are commonly used in high-performance computing and datacenters. Within each architecture, a processor-interconnect is used for communication between the different sockets and examples of such interconnect include Intel {QPI} and {AMD} {HyperTransport}. In this work, we explore the impact of the processor-interconnect on overall performance-in particular, we explore the impact on performance fairness from the processor-interconnect arbitration. It is well known that locally-fair arbitration does not guarantee globally-fair bandwidth sharing as closer nodes receive more bandwidth in a multi-hop network. However, this paper is the first to demonstrate the opposite can occur in a commodity {NUMA} servers where remote nodes receive higher bandwidth (and perform better). This problem occurs because router micro-architectures for processor-interconnects commonly employ external concentration. While accessing remote memory can occur in any {NUMA} system, performance unfairness (or performance variation) is more critical in cloud computing and virtual machines with shared resources. We demonstrate how this unfairness creates significant performance variation when executing workload on the Xen virtualization platform. We then provide analysis using synthetic workloads to better understand the source of unfairness.},
	pages = {26--29},
	number = {1},
	journaltitle = {{IEEE} Computer Architecture Letters},
	author = {Song, Wonjun and Jung, Hyung-Joon and Ahn, Jung Ho and Lee, Jae W. and Kim, John},
	date = {2017-01},
	note = {Conference Name: {IEEE} Computer Architecture Letters},
	keywords = {{AMD} {HyperTransport}, Bandwidth, cloud computing, globally-fair bandwidth sharing, high-performance computing, Intel {QPI}, locally-fair arbitration, memory architecture, Micromechanical devices, multihop network, Multiprocessor interconnection, nonuniform memory access system architectures, {NUMA}, {NUMA} system architecture, parallel processing, performance unfairness evaluation, processor-interconnect, processor-interconnect arbitration, processor-interconnects, router microarchitectures, Servers, shared resources, Sockets, System-on-chip, unfairness, virtual machines, Virtual machining, Xen virtualization platform},
}

@report{burks_preliminary_1946,
	title = {Preliminary discussion of the logical design of an electronic computer instrument},
	url = {http://hdl.handle.net/2027.42/3972},
	author = {Burks, Arthur W. and Goldstine, Herman Heine and von Neumann, John},
	urldate = {2021-04-19},
	date = {1946-06-28},
}


@inproceedings{patterson_case_1988,
	location = {New York, {NY}, {USA}},
	title = {A case for redundant arrays of inexpensive disks ({RAID})},
	isbn = {978-0-89791-268-6},
	url = {https://doi.org/10.1145/50202.50214},
	doi = {10.1145/50202.50214},
	series = {{SIGMOD} '88},
	abstract = {Increasing performance of {CPUs} and memories will be squandered if not matched by a similar performance increase in I/O. While the capacity of Single Large Expensive Disks ({SLED}) has grown rapidly, the performance improvement of {SLED} has been modest. Redundant Arrays of Inexpensive Disks ({RAID}), based on the magnetic disk technology developed for personal computers, offers an attractive alternative to {SLED}, promising improvements of an order of magnitude in performance, reliability, power consumption, and scalability. This paper introduces five levels of {RAIDs}, giving their relative cost/performance, and compares {RAID} to an {IBM} 3380 and a Fujitsu Super Eagle.},
	pages = {109--116},
	booktitle = {Proceedings of the 1988 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {Association for Computing Machinery},
	author = {Patterson, David A. and Gibson, Garth and Katz, Randy H.},
	urldate = {2021-04-18},
	date = {1988-06-01},
}


@misc{zfs_on_disk_2006,
	title = {{ZFS} On-Disk Specification},
	url = {http://www.giis.co.in/Zfs_ondiskformat.pdf},
	publisher = {Sun Microsystems},
	urldate = {2021-04-20},
	date = {2006},
}

@misc{azaghal_diagram_2012,
	title = {Diagram of a binary hash tree.},
	url = {https://commons.wikimedia.org/wiki/File:Hash_Tree.svg},
	shorttitle = {English},
	author = {{Azaghal}},
	urldate = {2021-04-21},
	date = {2012-01-25},
}

% This is an ancient source, 
@incollection{gorman_describing_2004,
	location = {Upper Saddle River, {NJ}},
	title = {Describing Physical Memory},
	isbn = {978-0-13-145348-7},
	url = {https://www.kernel.org/doc/gorman/html/understand/understand005.html},
	series = {Bruce Perens' Open Source series},
	booktitle = {Understanding the Linux Virtual Memory Manager},
	publisher = {Prentice Hall},
	author = {Gorman, Mel},
	urldate = {2020-12-23},
	date = {2004},
}